{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Sentiment Analysis for Hindi Text\n",
    "\n",
    "### Introduction\n",
    "In this Sentiment Analysis project we will take dataset of movie reviews in English and use it to train the Model. And then we will take manual inputs in Hindi Language and use the trained model to predict the sentiment of the manual input as positive or negative. We have used NLTK(Natural Language ToolKit) and SKLearn(Scikit Learn) for the preprocessing and model training. Due to lack of sufficient dataset in Hindi language, the model is trained on English dataset and the testing is done on the Hindi inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "Engish dataset, stored in 'movie_review.csv', is read by using pandas. Unnecessary columns are dropped and it is stored in MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MR = pd.read_csv('movie_review.csv')\n",
    "MR = MR.drop(['fold_id','cv_tag','html_id','sent_id'],axis = 1)   #MR is movie review which is having 2 columns one is text and other is their polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  tag\n",
      "0  films adapted from comic books have had plenty...  pos\n",
      "1  for starters , it was created by alan moore ( ...  pos\n",
      "2  to say moore and campbell thoroughly researche...  pos\n",
      "                                                    text  tag\n",
      "26535  however , it does have a tendency to avoid thi...  pos\n",
      "33000  with the young cast , cool clothes , nice hair...  neg\n",
      "46897  robin williams plays chris nielsen , who dies ...  neg\n",
      "(64720, 2)\n"
     ]
    }
   ],
   "source": [
    "print(MR.head(3))\n",
    "MR  = MR.sample(frac=1)\n",
    "print(MR.head(3))\n",
    "print(MR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Preprocessing of data is done in this section which includes:<br>\n",
    "1. Converting into lower case\n",
    "2. Removing punctuations<br>\n",
    "3. Removing numbers\n",
    "4. Removing white spaces\n",
    "5. Removing stop words\n",
    "6. Using Lemmatizer\n",
    "7. Stemming the words using **Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_file,target):\n",
    "    import re\n",
    "    import string\n",
    "    \n",
    "    #Convert All into Lower Case\n",
    "    input_file[target] = input_file[target].str.lower()\n",
    "    \n",
    "    #Removing Punctuation\n",
    "    input_file[target] = input_file[target].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    \n",
    "    #Removing Numbers\n",
    "    input_file[target] = input_file[target].str.replace('\\d+', '')\n",
    "\n",
    "    #Removing White Spaces\n",
    "    input_file[target] = input_file[target].str.strip()\n",
    "    \n",
    "    #Tokenization\n",
    "    #Removal of Stop Words\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stemmer= PorterStemmer()\n",
    "        \n",
    "        \n",
    "    \n",
    "    tokenized = input_file[target].apply(lambda p: p.split())\n",
    "    #print(tokenized[0])\n",
    "    #print(type(tokenized))\n",
    "    for i in range(tokenized.size):\n",
    "        result = [j for j in tokenized[i] if not j in ENGLISH_STOP_WORDS]\n",
    "        tokenized[i] = result\n",
    "        \n",
    "    #tokenized = tokenized.apply(lambda p: [for i in p if not in i in ENGLISH_STOP_WORDS])\n",
    "    tokenized = tokenized.apply(lambda p: [lemmatizer.lemmatize(i) for i in p])\n",
    "    tokenized = tokenized.apply(lambda p : [stemmer.stem(i) for i in p])\n",
    "    for i in range(len(tokenized)):\n",
    "        tokenized[i] = ' '.join(tokenized[i])\n",
    "    input_file[target] = tokenized\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  tag\n",
      "26535  however , it does have a tendency to avoid thi...  pos\n",
      "33000  with the young cast , cool clothes , nice hair...  neg\n",
      "46897  robin williams plays chris nielsen , who dies ...  neg\n",
      "57392                  let's get someone else to do it .  neg\n",
      "11889  the costuming is exceptional , and the dialogu...  pos\n",
      "17440  more and more details seem to complicate the j...  pos\n",
      "51128  at least neill makes some effort to retain the...  neg\n",
      "16946  the similarities are very apparent from the in...  pos\n",
      "62239  i am too young to have seen star wars in the t...  neg\n",
      "50799  the action scenes are decent , but few are not...  neg\n",
      "                                                    text  tag\n",
      "26535  doe tendenc avoid thing shift focu elect campa...  pos\n",
      "33000  young cast cool cloth nice hair hip soundtrack...  neg\n",
      "46897  robin william play chri nielsen dy prematur st...  neg\n",
      "57392                                              let s  neg\n",
      "11889  costum except dialogu crisp effect s recogniz ...  pos\n",
      "17440                                 detail complic job  pos\n",
      "51128  neill make effort retain digniti charact gore ...  neg\n",
      "16946  similar appar inexplic phenomenon like plot my...  pos\n",
      "62239  young seen star war theater origin releas does...  neg\n",
      "50799                     action scene decent noteworthi  neg\n",
      "(64720, 2)\n"
     ]
    }
   ],
   "source": [
    "print(MR.head(10))\n",
    "preprocessing(MR,'text')\n",
    "print(MR.head(10))\n",
    "print(MR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:\n",
    "TF-IDF technique is used for the feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(input_file,target):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidfV = TfidfVectorizer(max_df = 0.9,min_df = 2,max_features = 800, stop_words = 'english')\n",
    "    vecTor=  tfidfV.fit_transform(input_file[target])\n",
    "    return vecTor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64720, 800)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = feature_extraction(MR,'text')\n",
    "print(Xtrain.shape)\n",
    "#print(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Ytrain = []\n",
    "for i in MR['tag']:\n",
    "    if str(i) == 'pos':\n",
    "        Ytrain.append(1)\n",
    "    else:\n",
    "        Ytrain.append(0)\n",
    "#print(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model\n",
    "Model has been trained by using Decision Tree Algorithm. Inbuilt library of Scikit Learn is used for implementing **Decision Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "model = DT()\n",
    "X_train,X_test,Y_train,Y_test = TTS(Xtrain,Ytrain,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9531314379892872\n"
     ]
    }
   ],
   "source": [
    "prid = model.predict(X_test)\n",
    "print(accuracy_score(Y_test,prid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Input in Hindi\n",
    "Here we are taking the input in Hindi language. The input is then preprocessed and given to the trained model. We will get output as positive and negative from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalConversion(get_inp):\n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    get_inp = translator.translate(text = get_inp,dest = 'en',src = 'hi')\n",
    "    get_inp = get_inp.text\n",
    "    print(get_inp)\n",
    "    df1 = pd.DataFrame({'text':[get_inp],'tag':[0]})\n",
    "    new_df = MR\n",
    "    df1 = df1.append(new_df,ignore_index = True)\n",
    "    preprocessing(df1,'text')\n",
    "    tstVec = feature_extraction(df1,'text')\n",
    "    v = tstVec[1]\n",
    "    x = model.predict(v)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Text in Hindi: एबीसीडी 2’ में ड्रामा की कमी है, इस कमी की वजह से ही सुरु और विन्नी का रोमांस भी नहीं उभर पाता।\n",
      "ABCD 2 'lack of drama, due to the reduction does not emerge romance of Suru and Vinny.\n",
      "\n",
      "Prediction is: Positive\n"
     ]
    }
   ],
   "source": [
    "get_inp = input(\"Enter Text in Hindi: \")\n",
    "output = finalConversion(get_inp)\n",
    "if output == 0:\n",
    "    output = \"Negative\"\n",
    "else:\n",
    "    output = \"Positive\"\n",
    "print(\"\\nPrediction is: \" +output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# भाऊत अच्छी मूवी है ये!\n",
    "# एबीसीडी 2’ में ड्रामा की कमी है, इस कमी की वजह से ही सुरु और विन्नी का रोमांस भी नहीं उभर पाता।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
