{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi Sentiment Analysis for Hindi Text\n",
    "\n",
    "### Introduction\n",
    "In this Sentiment Analysis project we will take dataset of movie reviews in English and use it to train the Model. And then we will take manual inputs in Hindi Language and use the trained model to predict the sentiment of the manual input as positive or negative. We have used NLTK(Natural Language ToolKit) and SKLearn(Scikit Learn) for the preprocessing and model training. Due to lack of sufficient dataset in Hindi language, the model is trained on English dataset and the testing is done on the Hindi inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "Engish dataset, stored in 'movie_review.csv', is read by using pandas. Unnecessary columns are dropped and it is stored in MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MR = pd.read_csv('movie_review.csv')\n",
    "MR = MR.drop(['fold_id','cv_tag','html_id','sent_id'],axis = 1)   #MR is movie review which is having 2 columns one is text and other is their polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  tag\n",
      "0  films adapted from comic books have had plenty...  pos\n",
      "1  for starters , it was created by alan moore ( ...  pos\n",
      "2  to say moore and campbell thoroughly researche...  pos\n",
      "                                                    text  tag\n",
      "14920  the character of stargher is an excellent role...  pos\n",
      "62603  and it misses its best possible opportunity fo...  neg\n",
      "12327  after dying in a car crash , on his birthday o...  pos\n",
      "(64720, 2)\n"
     ]
    }
   ],
   "source": [
    "print(MR.head(3))\n",
    "MR  = MR.sample(frac=1)\n",
    "print(MR.head(3))\n",
    "print(MR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Preprocessing of data is done in this section which includes:<br>\n",
    "1. Converting into lower case\n",
    "2. Removing punctuations<br>\n",
    "3. Removing numbers\n",
    "4. Removing white spaces\n",
    "5. Removing stop words\n",
    "6. Using Lemmatizer\n",
    "7. Stemming the words using **Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_file,target):\n",
    "    import re\n",
    "    import string\n",
    "    \n",
    "    #Convert All into Lower Case\n",
    "    input_file[target] = input_file[target].str.lower()\n",
    "    \n",
    "    #Removing Punctuation\n",
    "    input_file[target] = input_file[target].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    \n",
    "    #Removing Numbers\n",
    "    input_file[target] = input_file[target].str.replace('\\d+', '')\n",
    "\n",
    "    #Removing White Spaces\n",
    "    input_file[target] = input_file[target].str.strip()\n",
    "    \n",
    "    #Tokenization\n",
    "    #Removal of Stop Words\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stemmer= PorterStemmer()\n",
    "        \n",
    "        \n",
    "    \n",
    "    tokenized = input_file[target].apply(lambda p: p.split())\n",
    "    print(tokenized[0])\n",
    "    print(type(tokenized))\n",
    "    for i in range(tokenized.size):\n",
    "        result = [j for j in tokenized[i] if not j in ENGLISH_STOP_WORDS]\n",
    "        tokenized[i] = result\n",
    "        \n",
    "    #tokenized = tokenized.apply(lambda p: [for i in p if not in i in ENGLISH_STOP_WORDS])\n",
    "    tokenized = tokenized.apply(lambda p: [lemmatizer.lemmatize(i) for i in p])\n",
    "    tokenized = tokenized.apply(lambda p : [stemmer.stem(i) for i in p])\n",
    "    for i in range(len(tokenized)):\n",
    "        tokenized[i] = ' '.join(tokenized[i])\n",
    "    input_file[target] = tokenized\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  tag\n",
      "14920  the character of stargher is an excellent role...  pos\n",
      "62603  and it misses its best possible opportunity fo...  neg\n",
      "12327  after dying in a car crash , on his birthday o...  pos\n",
      "44135                                                [r]  neg\n",
      "62391                                     nothing else .  neg\n",
      "35221  so there she is , narrating the six-astronaut ...  neg\n",
      "48712  when ned's mother ( clarissa kaye ) is jailed ...  neg\n",
      "58527                                    be forewarned .  neg\n",
      "34716          what one tough cop lacks is originality .  neg\n",
      "12716  it has changed horror movies forever and spawn...  pos\n",
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', 'whether', 'they', 're', 'about', 'superheroes', 'batman', 'superman', 'spawn', 'or', 'geared', 'toward', 'kids', 'casper', 'or', 'the', 'arthouse', 'crowd', 'ghost', 'world', 'but', 'there', 's', 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before']\n",
      "<class 'pandas.core.series.Series'>\n",
      "                                                    text  tag\n",
      "14920  charact stargher excel role probabl lead bit t...  pos\n",
      "62603  miss best possibl opportun great farc ignor si...  neg\n",
      "12327  die car crash birthday day brook wake judgemen...  pos\n",
      "44135                                                  r  neg\n",
      "62391                                                     neg\n",
      "35221                narrat astronaut mission blow comet  neg\n",
      "48712  ned s mother clarissa kay jail fals charg abet...  neg\n",
      "58527                                           forewarn  neg\n",
      "34716                              tough cop lack origin  neg\n",
      "12716    chang horror movi forev spawn sequel truli good  pos\n",
      "(64720, 2)\n"
     ]
    }
   ],
   "source": [
    "print(MR.head(10))\n",
    "preprocessing(MR,'text')\n",
    "print(MR.head(10))\n",
    "print(MR.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:\n",
    "TF-IDF technique is used for the feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(input_file,target):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidfV = TfidfVectorizer(max_df = 0.9,min_df = 2,max_features = 800, stop_words = 'english')\n",
    "    vecTor=  tfidfV.fit_transform(input_file[target])\n",
    "    return vecTor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64720, 800)\n",
      "  (0, 103)\t0.28501806826914744\n",
      "  (0, 222)\t0.49741543603609617\n",
      "  (0, 576)\t0.37323486918855153\n",
      "  (0, 526)\t0.4265359903844406\n",
      "  (0, 380)\t0.4190045993226688\n",
      "  (0, 64)\t0.4177804588873954\n",
      "  (1, 436)\t0.29854352726740335\n",
      "  (1, 61)\t0.2351345236360206\n",
      "  (1, 515)\t0.28677164695484364\n",
      "  (1, 476)\t0.335764851357128\n",
      "  (1, 288)\t0.2405339436286383\n",
      "  (1, 624)\t0.3073102535016016\n",
      "  (1, 234)\t0.2552191438795563\n",
      "  (1, 546)\t0.2523561661947294\n",
      "  (1, 63)\t0.24522113408474402\n",
      "  (1, 611)\t0.24719444027966347\n",
      "  (1, 477)\t0.2977077845312049\n",
      "  (1, 337)\t0.3354595761452148\n",
      "  (1, 761)\t0.23893375891293042\n",
      "  (2, 175)\t0.4901628623575851\n",
      "  (2, 89)\t0.49038153912511334\n",
      "  (2, 155)\t0.4104708448780358\n",
      "  (2, 110)\t0.46918061001047556\n",
      "  (2, 389)\t0.3614547760730428\n",
      "  (5, 437)\t1.0\n",
      "  :\t:\n",
      "  (64713, 660)\t0.38405829147720616\n",
      "  (64713, 44)\t0.3589066785199378\n",
      "  (64713, 49)\t0.7019034402864084\n",
      "  (64715, 448)\t0.49936967002101784\n",
      "  (64715, 269)\t0.8663890192419914\n",
      "  (64716, 448)\t0.16317138351525579\n",
      "  (64716, 712)\t0.20290382739154286\n",
      "  (64716, 366)\t0.2820001576655926\n",
      "  (64716, 683)\t0.29235337459402455\n",
      "  (64716, 178)\t0.264476485454617\n",
      "  (64716, 494)\t0.6909467143665492\n",
      "  (64716, 740)\t0.32107002182983707\n",
      "  (64716, 716)\t0.3417154071760291\n",
      "  (64717, 19)\t0.39946095560101075\n",
      "  (64717, 682)\t0.39750950874961205\n",
      "  (64717, 767)\t0.31769737343279275\n",
      "  (64717, 661)\t0.3030607759141786\n",
      "  (64717, 683)\t0.41013563355991967\n",
      "  (64717, 335)\t0.39364978063426564\n",
      "  (64717, 537)\t0.4080052606415362\n",
      "  (64718, 174)\t1.0\n",
      "  (64719, 253)\t0.46285252609885436\n",
      "  (64719, 240)\t0.4593946689948465\n",
      "  (64719, 53)\t0.5076369868357132\n",
      "  (64719, 83)\t0.5630530763430589\n"
     ]
    }
   ],
   "source": [
    "Xtrain = feature_extraction(MR,'text')\n",
    "print(Xtrain.shape)\n",
    "print(Xtrain)\n",
    "#print(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Ytrain = []\n",
    "for i in MR['tag']:\n",
    "    if str(i) == 'pos':\n",
    "        Ytrain.append(1)\n",
    "    else:\n",
    "        Ytrain.append(0)\n",
    "#print(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model\n",
    "Model has been trained by using Decision Tree Algorithm. Inbuilt library of Scikit Learn is used for implementing **Decision Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "model = DT()\n",
    "X_train,X_test,Y_train,Y_test = TTS(Xtrain,Ytrain,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535949732179646\n"
     ]
    }
   ],
   "source": [
    "prid = model.predict(X_test)\n",
    "print(accuracy_score(Y_test,prid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Input in Hindi\n",
    "Here we are taking the input in Hindi language. The input is then preprocessed and given to the trained model. We will get output as positive and negative from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalConversion(get_inp):\n",
    "    from googletrans import Translator\n",
    "    translator = Translator()\n",
    "    get_inp = translator.translate(text = get_inp,dest = 'en',src = 'hi')\n",
    "    get_inp = get_inp.text\n",
    "    print(get_inp)\n",
    "    df1 = pd.DataFrame({'text':[get_inp],'tag':[0]})\n",
    "    new_df = MR\n",
    "    df1 = df1.append(new_df,ignore_index = True)\n",
    "    preprocessing(df1,'text')\n",
    "    tstVec = feature_extraction(df1,'text')\n",
    "    v = tstVec[1]\n",
    "    x = model.predict(v)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inp = input(\"Enter Text in Hindi: \")\n",
    "output = finalConversion(get_inp)\n",
    "if output == 0:\n",
    "    output = \"Negative\"\n",
    "else:\n",
    "    output = \"Positive\"\n",
    "print(\"\\nPrediction is: \" +output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# भाऊत अच्छी मूवी है ये!\n",
    "# एबीसीडी 2’ में ड्रामा की कमी है, इस कमी की वजह से ही सुरु और विन्नी का रोमांस भी नहीं उभर पाता।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
